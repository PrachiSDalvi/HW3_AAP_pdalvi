{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26d9484-617f-4dc7-979e-07e4207f0b65",
   "metadata": {},
   "source": [
    "# Natural Language Processing using nltk\n",
    "\n",
    "Natural Language Processing, often known as NLP. In the field of artificial intelligence, and notably in machine learning, natural language processing is a hot topic. The reason being its its numerous uses in daily life.\n",
    "\n",
    "These applications include Chatbots, Language translation, Text Classification, Paragraph summarization, Spam filtering and many more. There are a few open-source NLP libraries, that do the job of processing text, like NLTK, Stanford NLP suite, Apache Open NLP, etc. I personally found NLTK to be the easy to understand. NLTK is a standard python library with prebuilt functions and utilities for the ease of use and implementation\n",
    "\n",
    "To begin with, we first install the nltk library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6806ad-bdfd-4415-80cd-6ee60869d2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (from click->nltk) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (from importlib-metadata->click->nltk) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (from importlib-metadata->click->nltk) (3.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938bc7d1-e7e9-4484-8e51-0239c26aa489",
   "metadata": {},
   "source": [
    "There are several nltk libraries which can be used with nltk. To use them, we need to download them by executing nltk.download()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "434ffa08-ae8b-4bcd-8658-4912e6b83ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d0328-5ec9-4e3e-90bf-b35b97f40f23",
   "metadata": {},
   "source": [
    "Once the download completes, we are set to go.\n",
    "\n",
    "## DATA PREPROCESSING - TEXT HANDLING\n",
    "\n",
    "As in any analytical processing the first step is to clean or prep our data, and few of the standard practices but not limited to are :\n",
    "\n",
    "Tokenization\n",
    "<br>\n",
    "Punctuation removal\n",
    "<br>\n",
    "Stop words removal\n",
    "<br>\n",
    "Stemming\n",
    "<br>\n",
    "Lammatization etc.\n",
    "\n",
    "#### Tokenization\n",
    "Tokenization is the process of breaking text up into smaller chunks as per our requirements that may be at the sentence or word level. We will need the sent_tokenize and word_tokenize from ntlk to do that so we import them. Here, we just have a sample text that we will use to understand the basics of nltk.tokenize package and its utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e0cc67-3d48-4d17-bb22-491162b00824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I live in a flat with my family.', 'We have two bedrooms and a living room.', 'We have a garden and we have some flowers there.', \"In weekdays I arrive home at five o'clock and I have lunch.\", 'Then I do my homework and go to bed.', \"I had a computer but now it doesn't work.\", 'I have a brother and a sister and I think I am very lucky to live with them.', 'Sometimes, our relatives visit us.', 'Our flat becomes very crowded sometimes but I like it.', 'What do you think?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "str1 = \"I live in a flat with my family. We have two bedrooms and a living room. We have a garden and we have some flowers there. In weekdays I arrive home at five o'clock and I have lunch. Then I do my homework and go to bed. I had a computer but now it doesn't work. I have a brother and a sister and I think I am very lucky to live with them. Sometimes, our relatives visit us. Our flat becomes very crowded sometimes but I like it. What do you think?\"\n",
    "print(sent_tokenize(str1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2c3207-809e-4f71-adf7-7655614e4ce2",
   "metadata": {},
   "source": [
    "As we see from the output the sent_tokenize, splits the data/paragraph at sentence ending at either ? or .(fullstop) . However, the word_tokenize submodule splits the data into each word token on whitepaces, fullstops and commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48fc863d-83d9-4b13-8d80-2a2d26e618f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'live', 'in', 'a', 'flat', 'with', 'my', 'family', '.', 'We', 'have', 'two', 'bedrooms', 'and', 'a', 'living', 'room', '.', 'We', 'have', 'a', 'garden', 'and', 'we', 'have', 'some', 'flowers', 'there', '.', 'In', 'weekdays', 'I', 'arrive', 'home', 'at', 'five', \"o'clock\", 'and', 'I', 'have', 'lunch', '.', 'Then', 'I', 'do', 'my', 'homework', 'and', 'go', 'to', 'bed', '.', 'I', 'had', 'a', 'computer', 'but', 'now', 'it', 'does', \"n't\", 'work', '.', 'I', 'have', 'a', 'brother', 'and', 'a', 'sister', 'and', 'I', 'think', 'I', 'am', 'very', 'lucky', 'to', 'live', 'with', 'them', '.', 'Sometimes', ',', 'our', 'relatives', 'visit', 'us', '.', 'Our', 'flat', 'becomes', 'very', 'crowded', 'sometimes', 'but', 'I', 'like', 'it', '.', 'What', 'do', 'you', 'think', '?']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(str1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec034499-5954-454b-a0d8-946462c198a2",
   "metadata": {},
   "source": [
    "The wordpunct_tokenize will further consider other punctuations in the sentence like the apostrphe(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd33f4d-77e3-45ef-ab9c-ed245b64acd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'live', 'in', 'a', 'flat', 'with', 'my', 'family', '.', 'We', 'have', 'two', 'bedrooms', 'and', 'a', 'living', 'room', '.', 'We', 'have', 'a', 'garden', 'and', 'we', 'have', 'some', 'flowers', 'there', '.', 'In', 'weekdays', 'I', 'arrive', 'home', 'at', 'five', 'o', \"'\", 'clock', 'and', 'I', 'have', 'lunch', '.', 'Then', 'I', 'do', 'my', 'homework', 'and', 'go', 'to', 'bed', '.', 'I', 'had', 'a', 'computer', 'but', 'now', 'it', 'doesn', \"'\", 't', 'work', '.', 'I', 'have', 'a', 'brother', 'and', 'a', 'sister', 'and', 'I', 'think', 'I', 'am', 'very', 'lucky', 'to', 'live', 'with', 'them', '.', 'Sometimes', ',', 'our', 'relatives', 'visit', 'us', '.', 'Our', 'flat', 'becomes', 'very', 'crowded', 'sometimes', 'but', 'I', 'like', 'it', '.', 'What', 'do', 'you', 'think', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "print(wordpunct_tokenize(str1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0856c5ba-119f-4320-9b3f-5ce29ff422ab",
   "metadata": {},
   "source": [
    "A RegexpTokenizer splits a string into substrings using a regular expression. For example, the following tokenizer forms tokens out of alphabetic sequences, money expressions, and any other non-whitespace sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb69a8b-189a-4f0a-82a2-320abe394c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', 'I', 'am', 'excited', 'that', 'good', 'muffins', 'cost', '3', '88', 'in', 'New', 'York', 'Please', 'buy', 'me', 'two', 'of', 'them', 'Thanks']\n",
      "['Wow', '!', 'I', 'am', 'excited', 'that', 'good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "result = tokenizer.tokenize(\"Wow! I am excited that good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\")\n",
    "print(result)\n",
    "\n",
    "\n",
    "#Compared to wordpunct_tokenize function\n",
    "print(wordpunct_tokenize(\"Wow! I am excited that good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db9cdb8-927a-412f-9d14-1cdfe7e8e3b5",
   "metadata": {},
   "source": [
    "#### Stopword\n",
    "Stopwords are words that are very common in human language but are generally not useful because they represent particularly common words such as “the”, “of”, and “to”. Stopword() removes the predefined stop words from a piece of text:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99dfab91-8b0c-4dae-834c-1bfc1e79407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ed56625-93e9-4798-bb76-2ee068e47768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words\n",
      "{\"it's\", 'theirs', 'if', 'with', 'how', 'own', 'o', 'having', 'this', 'our', 'who', 'haven', 'doing', 'on', 'when', 'their', 'from', 'off', \"wouldn't\", 'no', \"she's\", 'it', 'doesn', 'be', 's', 'had', 'yourself', 'between', 'than', 'in', 'above', 'yours', 'my', 'but', 'are', \"hadn't\", \"won't\", 'i', 'shan', 'of', 'll', 'that', 'he', 'they', 'does', 'against', \"weren't\", 'over', 'she', \"doesn't\", 'shouldn', 'very', 'yourselves', 'you', 'we', 'didn', \"aren't\", 'will', \"you'll\", \"you're\", \"that'll\", 'd', 'won', \"wasn't\", 'at', 'while', 'so', \"mightn't\", 'ourselves', 'being', 'and', 'nor', \"hasn't\", 'its', 'was', 'other', 'such', 'until', 'hasn', 'your', 'mustn', 'then', 'through', 'during', 'all', 'why', 're', 'ours', 'his', 'the', 'because', 'couldn', 'more', 'those', 'down', 'under', \"you'd\", 'by', 'once', \"haven't\", \"shan't\", 'weren', 'were', \"didn't\", 'him', 'before', 'been', 'whom', 'ain', 'is', 'now', \"you've\", 'mightn', 'any', 'each', \"mustn't\", 'for', 'both', 'most', 'can', 'there', 't', 'isn', 'again', 'needn', 'has', 'here', 'aren', 'herself', 'have', 'what', 'them', 'only', \"couldn't\", 'itself', 'wasn', \"needn't\", 'these', 'few', 'into', 'am', \"shouldn't\", 'just', 'a', 'himself', 'y', 'did', 'about', \"don't\", 'an', 'her', 'after', 'not', 'myself', 'm', 'some', 'wouldn', 'themselves', 'up', 'or', 'too', 'where', 'don', 'which', 'hadn', 'further', 'below', 'same', 'should', 'to', \"should've\", 'out', 'hers', 'do', \"isn't\", 'ma', 'as', 've', 'me'}\n",
      "\n",
      "Original Text\n",
      "['I', 'live', 'in', 'a', 'flat', 'with', 'my', 'family', '.', 'We', 'have', 'two', 'bedrooms', 'and', 'a', 'living', 'room', '.', 'We', 'have', 'a', 'garden', 'and', 'we', 'have', 'some', 'flowers', 'there', '.', 'In', 'weekdays', 'I', 'arrive', 'home', 'at', 'five', \"o'clock\", 'and', 'I', 'have', 'lunch', '.', 'Then', 'I', 'do', 'my', 'homework', 'and', 'go', 'to', 'bed', '.', 'I', 'had', 'a', 'computer', 'but', 'now', 'it', 'does', \"n't\", 'work', '.', 'I', 'have', 'a', 'brother', 'and', 'a', 'sister', 'and', 'I', 'think', 'I', 'am', 'very', 'lucky', 'to', 'live', 'with', 'them', '.', 'Sometimes', ',', 'our', 'relatives', 'visit', 'us', '.', 'Our', 'flat', 'becomes', 'very', 'crowded', 'sometimes', 'but', 'I', 'like', 'it', '.', 'What', 'do', 'you', 'think', '?']\n",
      "\n",
      "Filtered Text\n",
      "['I', 'live', 'flat', 'family', '.', 'We', 'two', 'bedrooms', 'living', 'room', '.', 'We', 'garden', 'flowers', '.', 'In', 'weekdays', 'I', 'arrive', 'home', 'five', \"o'clock\", 'I', 'lunch', '.', 'Then', 'I', 'homework', 'go', 'bed', '.', 'I', 'computer', \"n't\", 'work', '.', 'I', 'brother', 'sister', 'I', 'think', 'I', 'lucky', 'live', '.', 'Sometimes', ',', 'relatives', 'visit', 'us', '.', 'Our', 'flat', 'becomes', 'crowded', 'sometimes', 'I', 'like', '.', 'What', 'think', '?']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words( 'english' ))\n",
    "print('Stop words')\n",
    "print(stop_words)\n",
    "\n",
    "word_tokens = word_tokenize(str1)\n",
    "\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print('\\nOriginal Text')\n",
    "print(word_tokens)\n",
    "print('\\nFiltered Text')\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24040ac-87e8-49e5-8fd0-cb4b6da4e9f3",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "There might be words in our data which have same root meaning but different forms or they may be in difference tense, for eg. live, lived, living, the base word for this is live. Stemming helps to find similarities between words with the same root words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe44898b-d795-4a8e-ac08-ee5ed0aa30bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n",
      "['i', 'live', 'in', 'a', 'flat', 'with', 'my', 'famili', '.', 'we', 'have', 'two', 'bedroom', 'and', 'a', 'live', 'room', '.', 'we', 'have', 'a', 'garden', 'and', 'we', 'have', 'some', 'flower', 'there', '.', 'in', 'weekday', 'i', 'arriv', 'home', 'at', 'five', \"o'clock\", 'and', 'i', 'have', 'lunch', '.', 'then', 'i', 'do', 'my', 'homework', 'and', 'go', 'to', 'bed', '.', 'i', 'had', 'a', 'comput', 'but', 'now', 'it', 'doe', \"n't\", 'work', '.', 'i', 'have', 'a', 'brother', 'and', 'a', 'sister', 'and', 'i', 'think', 'i', 'am', 'veri', 'lucki', 'to', 'live', 'with', 'them', '.', 'sometim', ',', 'our', 'rel', 'visit', 'us', '.', 'our', 'flat', 'becom', 'veri', 'crowd', 'sometim', 'but', 'i', 'like', 'it', '.', 'what', 'do', 'you', 'think', '?']\n"
     ]
    }
   ],
   "source": [
    "#STEMMING\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "stem_word = []\n",
    "for w in word_tokens:\n",
    "    stem_word.append(ps.stem(w))\n",
    "    \n",
    "print(stem_word)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51358797-8e80-4c2b-9edd-03a423b6691b",
   "metadata": {},
   "source": [
    "Stemming works on standalone word without understanding its refernce in the sentence, foreg. in our str1 data the second sentence have living room and stemming converted it to live which is not correct with the context of the sentence. So the accuracy of stemming is not too reliable.\n",
    "\n",
    "#### Lemmatization\n",
    "Next we see Lemmatization, It is the process of combining a word's several forms into a single unit for analysis. Similar to stemming, however, lemmatization adds context to the words. As a result, it ties words with related meanings together, lemmatization is preferred over Stemming for this very reason. WordNetLemmatizer is the module in the nltk.stem that is used for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "817d6346-7433-4d39-9fa6-9b8296dff266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'live', 'in', 'a', 'flat', 'with', 'my', 'family', '.', 'We', 'have', 'two', 'bedroom', 'and', 'a', 'living', 'room', '.', 'We', 'have', 'a', 'garden', 'and', 'we', 'have', 'some', 'flower', 'there', '.', 'In', 'weekday', 'I', 'arrive', 'home', 'at', 'five', \"o'clock\", 'and', 'I', 'have', 'lunch', '.', 'Then', 'I', 'do', 'my', 'homework', 'and', 'go', 'to', 'bed', '.', 'I', 'had', 'a', 'computer', 'but', 'now', 'it', 'doe', \"n't\", 'work', '.', 'I', 'have', 'a', 'brother', 'and', 'a', 'sister', 'and', 'I', 'think', 'I', 'am', 'very', 'lucky', 'to', 'live', 'with', 'them', '.', 'Sometimes', ',', 'our', 'relative', 'visit', 'u', '.', 'Our', 'flat', 'becomes', 'very', 'crowded', 'sometimes', 'but', 'I', 'like', 'it', '.', 'What', 'do', 'you', 'think', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lem_word = []\n",
    "for w in word_tokens:\n",
    "    lem_word.append(lemmatizer.lemmatize(w))\n",
    "    \n",
    "print(lem_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eb8a2d-31b5-4047-9a18-532febceb947",
   "metadata": {},
   "source": [
    "\n",
    "#### Frequency Distribution\n",
    "once, we have found the root words we can find the frequency of each word in our str1 data using the FreqDist() from the nltk. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa498c96-f6e7-4de3-91f5-201e2415259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:9\n",
      "live:2\n",
      "in:1\n",
      "a:6\n",
      "flat:2\n",
      "with:2\n",
      "my:2\n",
      "family:1\n",
      ".:9\n",
      "We:2\n",
      "have:5\n",
      "two:1\n",
      "bedroom:1\n",
      "and:6\n",
      "living:1\n",
      "room:1\n",
      "garden:1\n",
      "we:1\n",
      "some:1\n",
      "flower:1\n",
      "there:1\n",
      "In:1\n",
      "weekday:1\n",
      "arrive:1\n",
      "home:1\n",
      "at:1\n",
      "five:1\n",
      "o'clock:1\n",
      "lunch:1\n",
      "Then:1\n",
      "do:2\n",
      "homework:1\n",
      "go:1\n",
      "to:2\n",
      "bed:1\n",
      "had:1\n",
      "computer:1\n",
      "but:2\n",
      "now:1\n",
      "it:2\n",
      "doe:1\n",
      "n't:1\n",
      "work:1\n",
      "brother:1\n",
      "sister:1\n",
      "think:2\n",
      "am:1\n",
      "very:2\n",
      "lucky:1\n",
      "them:1\n",
      "Sometimes:1\n",
      ",:1\n",
      "our:1\n",
      "relative:1\n",
      "visit:1\n",
      "u:1\n",
      "Our:1\n",
      "becomes:1\n",
      "crowded:1\n",
      "sometimes:1\n",
      "like:1\n",
      "What:1\n",
      "you:1\n",
      "?:1\n"
     ]
    }
   ],
   "source": [
    "frequency = nltk.FreqDist(lem_word) \n",
    "for key,val in frequency.items(): \n",
    "    print (str(key) + ':' + str(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73614de-d1f3-4f4c-a104-177d75a53f18",
   "metadata": {},
   "source": [
    "#### WordNet\n",
    "\n",
    "Wordnet is an English database for lexical which was based on the NLTK corpus reader. It can be used to look for word definitions, synonyms, and antonyms. It’s best described as an English dictionary with a semantic focus. The import command is used to bring it into the system. Because Wordnet is a corpus, it is pulled from the ntlk.corpus directory.\n",
    "\n",
    "Synset — “synonym set” — a collection of synonymous words. A name is all assigned to each Synset. Lemmas are the words found in a Synset. The function wordnet.synsets (‘word’) provides an array containing all of the Synsets associated with the word put in as an argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01b56287-2ac5-47e2-b3eb-2587887993d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('see.n.01'),\n",
       " Synset('see.v.01'),\n",
       " Synset('understand.v.02'),\n",
       " Synset('witness.v.02'),\n",
       " Synset('visualize.v.01'),\n",
       " Synset('see.v.05'),\n",
       " Synset('learn.v.02'),\n",
       " Synset('watch.v.03'),\n",
       " Synset('meet.v.01'),\n",
       " Synset('determine.v.08'),\n",
       " Synset('see.v.10'),\n",
       " Synset('see.v.11'),\n",
       " Synset('see.v.12'),\n",
       " Synset('visit.v.01'),\n",
       " Synset('attend.v.02'),\n",
       " Synset('see.v.15'),\n",
       " Synset('go_steady.v.01'),\n",
       " Synset('see.v.17'),\n",
       " Synset('see.v.18'),\n",
       " Synset('see.v.19'),\n",
       " Synset('examine.v.02'),\n",
       " Synset('experience.v.01'),\n",
       " Synset('see.v.22'),\n",
       " Synset('see.v.23'),\n",
       " Synset('interpret.v.01')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('See')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3542862a-7834-4071-9c3e-47c04881cee7",
   "metadata": {},
   "source": [
    "The output means that word see has 25 possible context, 1 out of which is noun and other are all verbs, it also shows how many different meaning 'see' word has. Next we are passing the pos argument which lets you constrain the part of speech of the word, in this case we are checking all verb word synsets for see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d140c33-a0aa-4914-ba2a-b3c2cc5dd098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('see.v.01'), Synset('understand.v.02'), Synset('witness.v.02'), Synset('visualize.v.01'), Synset('see.v.05'), Synset('learn.v.02'), Synset('watch.v.03'), Synset('meet.v.01'), Synset('determine.v.08'), Synset('see.v.10'), Synset('see.v.11'), Synset('see.v.12'), Synset('visit.v.01'), Synset('attend.v.02'), Synset('see.v.15'), Synset('go_steady.v.01'), Synset('see.v.17'), Synset('see.v.18'), Synset('see.v.19'), Synset('examine.v.02'), Synset('experience.v.01'), Synset('see.v.22'), Synset('see.v.23'), Synset('interpret.v.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "syns = wn.synsets('See', pos = wn.VERB)\n",
    "\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b8e76-3a69-49ae-b26b-5b66cae1bd6b",
   "metadata": {},
   "source": [
    "lemma_names() is used to return all lemma (group of different inflected form of a word) names of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9389bb99-0c58-4b76-868c-4fd844c5dbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learn', 'hear', 'get_word', 'get_wind', 'pick_up', 'find_out', 'get_a_line', 'discover', 'see']\n"
     ]
    }
   ],
   "source": [
    "print(syns[5].lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5410fab-b222-4f31-badd-13c43a8bd7af",
   "metadata": {},
   "source": [
    "definition() as the name represents provides definition of the word, here we are checking the definition of first synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d985d535-809e-4394-ba42-42347eed452a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perceive by sight or have the power to perceive by sight\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d0b90-677b-4c33-a2b0-69ad142081fd",
   "metadata": {},
   "source": [
    "examples() gives examples of the word in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e15d702e-2045-4096-96c9-a2dd0e75de3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You have to be a good observer to see all the details', 'Can you see the bird in that tree?', 'He is blind--he cannot see']\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2607a85f-ca63-405d-a9c0-58f05a49f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.corpus import subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28584e53-8d43-4a2a-86da-a18d57e43ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "len(subj_docs), len(obj_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42ec4a63-1d02-4053-976d-6da7f4141f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['smart',\n",
       "  'and',\n",
       "  'alert',\n",
       "  ',',\n",
       "  'thirteen',\n",
       "  'conversations',\n",
       "  'about',\n",
       "  'one',\n",
       "  'thing',\n",
       "  'is',\n",
       "  'a',\n",
       "  'small',\n",
       "  'gem',\n",
       "  '.'],\n",
       " 'subj')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e2785-49ee-4fe2-8fc2-8704c2635ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039ed5ff-5e92-416f-9c9e-fbe52ad2cfd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc6aa1d-30cd-4f89-b884-597069049653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aap]",
   "language": "python",
   "name": "conda-env-aap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
