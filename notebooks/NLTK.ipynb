{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26d9484-617f-4dc7-979e-07e4207f0b65",
   "metadata": {},
   "source": [
    "Natural Language Processing,# Natural Language Processing using nltk\n",
    "\n",
    "Natural Language Processing, often known as NLP. In the field of artificial intelligence, and notably in machine learning, natural language processing is a hot topic. The reason being its numerous uses in daily life.\n",
    "\n",
    "These applications include Chatbots, Language translation, Text Classification, Paragraph summarization, Spam filtering and many more. There are a few open-source NLP libraries, that do the job of processing text, like NLTK, Stanford NLP suite, Apache Open NLP, etc. I personally found NLTK to be the easy to understand. NLTK is a standard python library with prebuilt functions and utilities for the ease of use and implementation\n",
    "\n",
    "To begin with, we first install the nltk library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea6806ad-bdfd-4415-80cd-6ee60869d2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (from click->nltk) (4.11.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (from importlib-metadata->click->nltk) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\prachi\\anaconda3\\envs\\aap\\lib\\site-packages (from importlib-metadata->click->nltk) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938bc7d1-e7e9-4484-8e51-0239c26aa489",
   "metadata": {},
   "source": [
    "There are several nltk libraries which can be used with nltk. To use them, we need to download them by executing nltk.download()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "434ffa08-ae8b-4bcd-8658-4912e6b83ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d0328-5ec9-4e3e-90bf-b35b97f40f23",
   "metadata": {},
   "source": [
    "Once the download completes, we are set to go.\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "As in any analytical processing the first step is to clean or prep our data, and few of the standard practices but not limited to are :\n",
    "\n",
    "Tokenization\n",
    "<br>\n",
    "Punctuation removal\n",
    "<br>\n",
    "Stop words removal\n",
    "<br>\n",
    "Stemming\n",
    "<br>\n",
    "Lammatization etc.\n",
    "\n",
    "#### Tokenization\n",
    "Tokenization is the process of breaking text up into smaller chunks as per our requirements that may be at the sentence or word level. We will need the sent_tokenize and word_tokenize from ntlk to do that so we import them. Here, we just have a sample text that we will use to understand the basics of nltk.tokenize package and its utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03e0cc67-3d48-4d17-bb22-491162b00824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I live in a flat with my family.', 'We have two bedrooms and a living room.', 'We have a garden and we have some flowers there.', \"In weekdays I arrive home at five o'clock and I have lunch.\", 'Then I do my homework and go to bed.', \"I had a computer but now it doesn't work.\", 'I have a brother and a sister and I think I am very lucky to live with them.', 'Sometimes, our relatives visit us.', 'Our flat becomes very crowded sometimes but I like it.', 'What do you think?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "str1 = \"I live in a flat with my family. We have two bedrooms and a living room. We have a garden and we have some flowers there. In weekdays I arrive home at five o'clock and I have lunch. Then I do my homework and go to bed. I had a computer but now it doesn't work. I have a brother and a sister and I think I am very lucky to live with them. Sometimes, our relatives visit us. Our flat becomes very crowded sometimes but I like it. What do you think?\"\n",
    "print(sent_tokenize(str1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2c3207-809e-4f71-adf7-7655614e4ce2",
   "metadata": {},
   "source": [
    "As we see from the output the sent_tokenize, splits the data/paragraph at sentence ending at either ? or .(fullstop) . However, the word_tokenize submodule splits the data into each word token on whitepaces, fullstops and commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48fc863d-83d9-4b13-8d80-2a2d26e618f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'live', 'in', 'a', 'flat', 'with', 'my', 'family', '.', 'We', 'have', 'two', 'bedrooms', 'and', 'a', 'living', 'room', '.', 'We', 'have', 'a', 'garden', 'and', 'we', 'have', 'some', 'flowers', 'there', '.', 'In', 'weekdays', 'I', 'arrive', 'home', 'at', 'five', \"o'clock\", 'and', 'I', 'have', 'lunch', '.', 'Then', 'I', 'do', 'my', 'homework', 'and', 'go', 'to', 'bed', '.', 'I', 'had', 'a', 'computer', 'but', 'now', 'it', 'does', \"n't\", 'work', '.', 'I', 'have', 'a', 'brother', 'and', 'a', 'sister', 'and', 'I', 'think', 'I', 'am', 'very', 'lucky', 'to', 'live', 'with', 'them', '.', 'Sometimes', ',', 'our', 'relatives', 'visit', 'us', '.', 'Our', 'flat', 'becomes', 'very', 'crowded', 'sometimes', 'but', 'I', 'like', 'it', '.', 'What', 'do', 'you', 'think', '?']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(str1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec034499-5954-454b-a0d8-946462c198a2",
   "metadata": {},
   "source": [
    "The wordpunct_tokenize will further consider other punctuations in the sentence like the apostrphe(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dd33f4d-77e3-45ef-ab9c-ed245b64acd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'live', 'in', 'a', 'flat', 'with', 'my', 'family', '.', 'We', 'have', 'two', 'bedrooms', 'and', 'a', 'living', 'room', '.', 'We', 'have', 'a', 'garden', 'and', 'we', 'have', 'some', 'flowers', 'there', '.', 'In', 'weekdays', 'I', 'arrive', 'home', 'at', 'five', 'o', \"'\", 'clock', 'and', 'I', 'have', 'lunch', '.', 'Then', 'I', 'do', 'my', 'homework', 'and', 'go', 'to', 'bed', '.', 'I', 'had', 'a', 'computer', 'but', 'now', 'it', 'doesn', \"'\", 't', 'work', '.', 'I', 'have', 'a', 'brother', 'and', 'a', 'sister', 'and', 'I', 'think', 'I', 'am', 'very', 'lucky', 'to', 'live', 'with', 'them', '.', 'Sometimes', ',', 'our', 'relatives', 'visit', 'us', '.', 'Our', 'flat', 'becomes', 'very', 'crowded', 'sometimes', 'but', 'I', 'like', 'it', '.', 'What', 'do', 'you', 'think', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "print(wordpunct_tokenize(str1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0856c5ba-119f-4320-9b3f-5ce29ff422ab",
   "metadata": {},
   "source": [
    "A RegexpTokenizer splits a string into substrings using a regular expression. For example, the following tokenizer forms tokens out of alphabetic sequences, money expressions, and any other non-whitespace sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bb69a8b-189a-4f0a-82a2-320abe394c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow', 'I', 'am', 'excited', 'that', 'good', 'muffins', 'cost', '3', '88', 'in', 'New', 'York', 'Please', 'buy', 'me', 'two', 'of', 'them', 'Thanks']\n",
      "['Wow', '!', 'I', 'am', 'excited', 'that', 'good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "result = tokenizer.tokenize(\"Wow! I am excited that good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\")\n",
    "print(result)\n",
    "\n",
    "\n",
    "#Compared to wordpunct_tokenize function\n",
    "print(wordpunct_tokenize(\"Wow! I am excited that good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db9cdb8-927a-412f-9d14-1cdfe7e8e3b5",
   "metadata": {},
   "source": [
    "#### Stopword\n",
    "Stopwords are words that are very common in human language but are generally not useful because they represent particularly common words such as “the”, “of”, and “to”. Stopword() removes the predefined stop words from a piece of text:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99dfab91-8b0c-4dae-834c-1bfc1e79407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ed56625-93e9-4798-bb76-2ee068e47768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words\n",
      "{'re', 'these', 'same', \"doesn't\", \"didn't\", \"weren't\", 'can', 'aren', 'do', 'we', \"she's\", 'hasn', 'shan', 'and', 'hadn', 'our', 'm', 'my', \"hadn't\", 'further', 'whom', 'are', 'am', 'needn', 'from', 'down', 'himself', 'as', 'ours', 'i', 'so', 'had', 'than', 'did', 'few', 'don', 'own', \"should've\", 'it', 'd', 'very', 'what', 'you', 'her', 'y', 'once', 'of', 'haven', 'me', 'during', 'ma', 'this', 'their', 'him', 'nor', \"shan't\", 'over', \"you'll\", 'doesn', 'was', 'both', 'will', 'why', 't', 'in', 'at', 've', 'only', \"isn't\", 'they', 'such', 'some', 'itself', 'for', \"wouldn't\", 'a', 'because', 'more', 'o', 'all', 'them', 'on', 'but', \"wasn't\", 'or', 'again', 'your', 'no', 'until', 'doing', \"it's\", 'other', 'yourselves', 'were', 'shouldn', 'not', 'his', 'too', 'off', 'below', 'while', 'being', 'mightn', 'having', \"couldn't\", 'couldn', 'yourself', 'does', \"you'd\", 'won', 'which', 'have', 'into', 'through', \"you've\", \"haven't\", 'against', 'should', 'to', 'is', 'hers', 'didn', 'now', 'herself', 'each', \"aren't\", 'themselves', 'ourselves', 'out', \"needn't\", \"you're\", 'up', 'has', 'before', 'its', 'with', 'then', 'if', 'wouldn', 'who', 'after', 'how', 'under', 's', \"won't\", 'myself', 'by', 'weren', 'between', 'that', 'she', 'ain', 'above', 'he', \"shouldn't\", 'be', 'when', 'where', 'just', 'isn', 'most', \"don't\", 'those', 'theirs', 'there', 'the', 'yours', 'here', 'wasn', \"mightn't\", 'll', \"that'll\", 'any', 'mustn', 'about', \"mustn't\", 'been', \"hasn't\", 'an'}\n",
      "\n",
      "Original Text\n",
      "['I', 'live', 'in', 'a', 'flat', 'with', 'my', 'family', '.', 'We', 'have', 'two', 'bedrooms', 'and', 'a', 'living', 'room', '.', 'We', 'have', 'a', 'garden', 'and', 'we', 'have', 'some', 'flowers', 'there', '.', 'In', 'weekdays', 'I', 'arrive', 'home', 'at', 'five', \"o'clock\", 'and', 'I', 'have', 'lunch', '.', 'Then', 'I', 'do', 'my', 'homework', 'and', 'go', 'to', 'bed', '.', 'I', 'had', 'a', 'computer', 'but', 'now', 'it', 'does', \"n't\", 'work', '.', 'I', 'have', 'a', 'brother', 'and', 'a', 'sister', 'and', 'I', 'think', 'I', 'am', 'very', 'lucky', 'to', 'live', 'with', 'them', '.', 'Sometimes', ',', 'our', 'relatives', 'visit', 'us', '.', 'Our', 'flat', 'becomes', 'very', 'crowded', 'sometimes', 'but', 'I', 'like', 'it', '.', 'What', 'do', 'you', 'think', '?']\n",
      "\n",
      "Filtered Text\n",
      "['I', 'live', 'flat', 'family', '.', 'We', 'two', 'bedrooms', 'living', 'room', '.', 'We', 'garden', 'flowers', '.', 'In', 'weekdays', 'I', 'arrive', 'home', 'five', \"o'clock\", 'I', 'lunch', '.', 'Then', 'I', 'homework', 'go', 'bed', '.', 'I', 'computer', \"n't\", 'work', '.', 'I', 'brother', 'sister', 'I', 'think', 'I', 'lucky', 'live', '.', 'Sometimes', ',', 'relatives', 'visit', 'us', '.', 'Our', 'flat', 'becomes', 'crowded', 'sometimes', 'I', 'like', '.', 'What', 'think', '?']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words( 'english' ))\n",
    "print('Stop words')\n",
    "print(stop_words)\n",
    "\n",
    "word_tokens = word_tokenize(str1)\n",
    "\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print('\\nOriginal Text')\n",
    "print(word_tokens)\n",
    "print('\\nFiltered Text')\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24040ac-87e8-49e5-8fd0-cb4b6da4e9f3",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "There might be words in our data which have same root meaning but different forms or they may be in different tense, for eg. live, lived, living, the base word for this is live. Stemming helps to find similarities between words with the same root words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe44898b-d795-4a8e-ac08-ee5ed0aa30bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n",
      "['i', 'live', 'in', 'a', 'flat', 'with', 'my', 'famili', '.', 'we', 'have', 'two', 'bedroom', 'and', 'a', 'live', 'room', '.', 'we', 'have', 'a', 'garden', 'and', 'we', 'have', 'some', 'flower', 'there', '.', 'in', 'weekday', 'i', 'arriv', 'home', 'at', 'five', \"o'clock\", 'and', 'i', 'have', 'lunch', '.', 'then', 'i', 'do', 'my', 'homework', 'and', 'go', 'to', 'bed', '.', 'i', 'had', 'a', 'comput', 'but', 'now', 'it', 'doe', \"n't\", 'work', '.', 'i', 'have', 'a', 'brother', 'and', 'a', 'sister', 'and', 'i', 'think', 'i', 'am', 'veri', 'lucki', 'to', 'live', 'with', 'them', '.', 'sometim', ',', 'our', 'rel', 'visit', 'us', '.', 'our', 'flat', 'becom', 'veri', 'crowd', 'sometim', 'but', 'i', 'like', 'it', '.', 'what', 'do', 'you', 'think', '?']\n"
     ]
    }
   ],
   "source": [
    "#STEMMING\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "stem_word = []\n",
    "for w in word_tokens:\n",
    "    stem_word.append(ps.stem(w))\n",
    "    \n",
    "print(stem_word)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51358797-8e80-4c2b-9edd-03a423b6691b",
   "metadata": {},
   "source": [
    "Stemming works on standalone word without understanding its refernce in the sentence, foreg. in our str1 data the second sentence have living room and stemming converted it to live which is not correct with the context of the sentence. So the accuracy of stemming is not too reliable.\n",
    "\n",
    "#### Lemmatization\n",
    "Next we see Lemmatization, It is the process of combining a word's several forms into a single unit for analysis. Similar to stemming, however, lemmatization adds context to the words. As a result, it ties words with related meanings together, lemmatization is preferred over Stemming for this very reason. WordNetLemmatizer is the module in the nltk.stem that is used for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "817d6346-7433-4d39-9fa6-9b8296dff266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'live', 'in', 'a', 'flat', 'with', 'my', 'family', '.', 'We', 'have', 'two', 'bedroom', 'and', 'a', 'living', 'room', '.', 'We', 'have', 'a', 'garden', 'and', 'we', 'have', 'some', 'flower', 'there', '.', 'In', 'weekday', 'I', 'arrive', 'home', 'at', 'five', \"o'clock\", 'and', 'I', 'have', 'lunch', '.', 'Then', 'I', 'do', 'my', 'homework', 'and', 'go', 'to', 'bed', '.', 'I', 'had', 'a', 'computer', 'but', 'now', 'it', 'doe', \"n't\", 'work', '.', 'I', 'have', 'a', 'brother', 'and', 'a', 'sister', 'and', 'I', 'think', 'I', 'am', 'very', 'lucky', 'to', 'live', 'with', 'them', '.', 'Sometimes', ',', 'our', 'relative', 'visit', 'u', '.', 'Our', 'flat', 'becomes', 'very', 'crowded', 'sometimes', 'but', 'I', 'like', 'it', '.', 'What', 'do', 'you', 'think', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lem_word = []\n",
    "for w in word_tokens:\n",
    "    lem_word.append(lemmatizer.lemmatize(w))\n",
    "    \n",
    "print(lem_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eb8a2d-31b5-4047-9a18-532febceb947",
   "metadata": {},
   "source": [
    "\n",
    "#### Frequency Distribution\n",
    "once, we have found the root words we can find the frequency of each word in our str1 data using the FreqDist() from the nltk. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa498c96-f6e7-4de3-91f5-201e2415259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:9\n",
      "live:2\n",
      "in:1\n",
      "a:6\n",
      "flat:2\n",
      "with:2\n",
      "my:2\n",
      "family:1\n",
      ".:9\n",
      "We:2\n",
      "have:5\n",
      "two:1\n",
      "bedroom:1\n",
      "and:6\n",
      "living:1\n",
      "room:1\n",
      "garden:1\n",
      "we:1\n",
      "some:1\n",
      "flower:1\n",
      "there:1\n",
      "In:1\n",
      "weekday:1\n",
      "arrive:1\n",
      "home:1\n",
      "at:1\n",
      "five:1\n",
      "o'clock:1\n",
      "lunch:1\n",
      "Then:1\n",
      "do:2\n",
      "homework:1\n",
      "go:1\n",
      "to:2\n",
      "bed:1\n",
      "had:1\n",
      "computer:1\n",
      "but:2\n",
      "now:1\n",
      "it:2\n",
      "doe:1\n",
      "n't:1\n",
      "work:1\n",
      "brother:1\n",
      "sister:1\n",
      "think:2\n",
      "am:1\n",
      "very:2\n",
      "lucky:1\n",
      "them:1\n",
      "Sometimes:1\n",
      ",:1\n",
      "our:1\n",
      "relative:1\n",
      "visit:1\n",
      "u:1\n",
      "Our:1\n",
      "becomes:1\n",
      "crowded:1\n",
      "sometimes:1\n",
      "like:1\n",
      "What:1\n",
      "you:1\n",
      "?:1\n"
     ]
    }
   ],
   "source": [
    "frequency = nltk.FreqDist(lem_word) \n",
    "for key,val in frequency.items(): \n",
    "    print (str(key) + ':' + str(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73614de-d1f3-4f4c-a104-177d75a53f18",
   "metadata": {},
   "source": [
    "#### WordNet\n",
    "\n",
    "Wordnet is an English database for lexical which was based on the NLTK corpus reader. It can be used to look for word definitions, synonyms, and antonyms. It’s best described as an English dictionary with a semantic focus. The import command is used to bring it into the system. Because Wordnet is a corpus, it is pulled from the ntlk.corpus directory.\n",
    "\n",
    "Synset — “synonym set” — a collection of synonymous words. A name is all assigned to each Synset. Lemmas are the words found in a Synset. The function wordnet.synsets (‘word’) provides an array containing all of the Synsets associated with the word put in as an argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01b56287-2ac5-47e2-b3eb-2587887993d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('see.n.01'),\n",
       " Synset('see.v.01'),\n",
       " Synset('understand.v.02'),\n",
       " Synset('witness.v.02'),\n",
       " Synset('visualize.v.01'),\n",
       " Synset('see.v.05'),\n",
       " Synset('learn.v.02'),\n",
       " Synset('watch.v.03'),\n",
       " Synset('meet.v.01'),\n",
       " Synset('determine.v.08'),\n",
       " Synset('see.v.10'),\n",
       " Synset('see.v.11'),\n",
       " Synset('see.v.12'),\n",
       " Synset('visit.v.01'),\n",
       " Synset('attend.v.02'),\n",
       " Synset('see.v.15'),\n",
       " Synset('go_steady.v.01'),\n",
       " Synset('see.v.17'),\n",
       " Synset('see.v.18'),\n",
       " Synset('see.v.19'),\n",
       " Synset('examine.v.02'),\n",
       " Synset('experience.v.01'),\n",
       " Synset('see.v.22'),\n",
       " Synset('see.v.23'),\n",
       " Synset('interpret.v.01')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('See')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3542862a-7834-4071-9c3e-47c04881cee7",
   "metadata": {},
   "source": [
    "The output means that word see has 25 possible context, 1 out of which is noun and other are all verbs, it also shows how many different meaning 'see' word has. Next we are passing the pos argument which lets you constrain the part of speech of the word, in this case we are checking all verb word synsets for see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d140c33-a0aa-4914-ba2a-b3c2cc5dd098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('see.v.01'), Synset('understand.v.02'), Synset('witness.v.02'), Synset('visualize.v.01'), Synset('see.v.05'), Synset('learn.v.02'), Synset('watch.v.03'), Synset('meet.v.01'), Synset('determine.v.08'), Synset('see.v.10'), Synset('see.v.11'), Synset('see.v.12'), Synset('visit.v.01'), Synset('attend.v.02'), Synset('see.v.15'), Synset('go_steady.v.01'), Synset('see.v.17'), Synset('see.v.18'), Synset('see.v.19'), Synset('examine.v.02'), Synset('experience.v.01'), Synset('see.v.22'), Synset('see.v.23'), Synset('interpret.v.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "syns = wn.synsets('See', pos = wn.VERB)\n",
    "\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b8e76-3a69-49ae-b26b-5b66cae1bd6b",
   "metadata": {},
   "source": [
    "lemma_names() is used to return all lemma (group of different inflected form of a word) names of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9389bb99-0c58-4b76-868c-4fd844c5dbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learn', 'hear', 'get_word', 'get_wind', 'pick_up', 'find_out', 'get_a_line', 'discover', 'see']\n"
     ]
    }
   ],
   "source": [
    "print(syns[5].lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5410fab-b222-4f31-badd-13c43a8bd7af",
   "metadata": {},
   "source": [
    "definition() as the name represents provides definition of the word, here we are checking the definition of first synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d985d535-809e-4394-ba42-42347eed452a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perceive by sight or have the power to perceive by sight\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d0b90-677b-4c33-a2b0-69ad142081fd",
   "metadata": {},
   "source": [
    "examples() gives examples of the word in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e15d702e-2045-4096-96c9-a2dd0e75de3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You have to be a good observer to see all the details', 'Can you see the bird in that tree?', 'He is blind--he cannot see']\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61cbd55-d93c-4a3b-a42f-840296c9ad03",
   "metadata": {},
   "source": [
    "### Sentiment Analysis with nltk\n",
    "\n",
    "Now that we have seen how to clean the data, its time to implement it and use it for analysis. One of the important analysis that can be done with nltk is Sentiment Analysis. Sentiment analysis is a technique used to determine the emotional tone or sentiment expressed in a text. It involves analyzing the words and phrases used in the text to identify the underlying sentiment, whether it is positive, negative, or neutral. \n",
    "\n",
    "VADER (Valence Aware Dictionary and sentiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attentive to sentiments expressed\n",
    "\n",
    "It is used for sentiment analysis of text which has both the polarities i.e. positive/negative. VADER is used to quantify how much of positive or negative emotion the text has and also the intensity of emotion. We will be using the SentimentIntensityAnalyzer object that will provide us with sentiment scores based on the words used. \n",
    "\n",
    "Similar to all the previous modules we start by importing the libraries and modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "798e2785-49ee-4fe2-8fc2-8704c2635ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68437d1-d1f6-4da6-8dca-5aab2978eb59",
   "metadata": {},
   "source": [
    "We start by reading our input file which has customer reviews and a flag for positive feedback, 1 if the sentiments were positive else 0.  the read_csv read the file, however, even after mentioning the separator character the column did not get split, so to split the column we use the split function of the dataframe by specifying the character \",\". Comma is a very common character in text based column, so we need to find the last comma in the dataframe to split the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa3869ae-be25-4936-86bd-ce40af6d4445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     reviewText,Positive\n",
      "0      This is a one of the best apps acording to a b...\n",
      "1      This is a pretty good version of the game for ...\n",
      "2      this is a really cool game. there are a bunch ...\n",
      "3      \"This is a silly game and can be frustrating, ...\n",
      "4      This is a terrific game on any pad. Hrs of fun...\n",
      "...                                                  ...\n",
      "19995  this app is fricken stupid.it froze on the kin...\n",
      "19996  Please add me!!!!! I need neighbors! Ginger101...\n",
      "19997  love it!  this game. is awesome. wish it had m...\n",
      "19998  I love love love this app on my side of fashio...\n",
      "19999  \"This game is a rip off. Here is a list of thi...\n",
      "\n",
      "[20000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the amazon review dataset\n",
    "\n",
    "df = pd.read_csv(\"./input/Review.csv\", sep = \",\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1260a16a-b5c1-4944-b6eb-1aa439d9e613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     reviewText,Positive Positive\n",
      "0      This is a one of the best apps acording to a b...        1\n",
      "1      This is a pretty good version of the game for ...        1\n",
      "2      this is a really cool game. there are a bunch ...        1\n",
      "3      \"This is a silly game and can be frustrating, ...        1\n",
      "4      This is a terrific game on any pad. Hrs of fun...        1\n",
      "...                                                  ...      ...\n",
      "19995  this app is fricken stupid.it froze on the kin...        0\n",
      "19996  Please add me!!!!! I need neighbors! Ginger101...        1\n",
      "19997  love it!  this game. is awesome. wish it had m...        1\n",
      "19998  I love love love this app on my side of fashio...        1\n",
      "19999  \"This game is a rip off. Here is a list of thi...        0\n",
      "\n",
      "[20000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df['Positive'] = df.iloc[:,0].str.split(',').str[-1]\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4683f92-b8fd-463a-821c-a04b0c0d6694",
   "metadata": {},
   "source": [
    "we divided the column, but the datatype of the column was python object, which we need to change to int so that we can use it further for comparing it to predicted value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b306e3ab-908f-466a-8268-68577bf9ea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "        ..\n",
      "19995    0\n",
      "19996    1\n",
      "19997    1\n",
      "19998    1\n",
      "19999    0\n",
      "Name: Positive, Length: 20000, dtype: object\n",
      "object\n",
      "int32\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[:,1])\n",
    "print(df['Positive'].dtypes)\n",
    "df['Positive'] = df['Positive'].astype(str).astype(int)\n",
    "print(df['Positive'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9564e965-b7bf-4ef1-872f-a3b19dfd189c",
   "metadata": {},
   "source": [
    "we are creating a function to preprocess/clean our data, by using tokenization, removing stopwords and lemmatization. In tokenization we are using the lower function to convert the text to lowercase as most of string function are case sensitive. we then pass our dataframe to transform and get text which is clean and will provide us with better future results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0eca28ab-feb6-4504-aeeb-64e3d225e25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText,Positive</th>\n",
       "      <th>Positive</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a one of the best apps acording to a b...</td>\n",
       "      <td>1</td>\n",
       "      <td>one best apps acording bunch people agree bomb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a pretty good version of the game for ...</td>\n",
       "      <td>1</td>\n",
       "      <td>pretty good version game free . lot different ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is a really cool game. there are a bunch ...</td>\n",
       "      <td>1</td>\n",
       "      <td>really cool game . bunch level find golden egg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"This is a silly game and can be frustrating, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>`` silly game frustrating , lot fun definitely...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a terrific game on any pad. Hrs of fun...</td>\n",
       "      <td>1</td>\n",
       "      <td>terrific game pad . hr fun . grandkids love . ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>this app is fricken stupid.it froze on the kin...</td>\n",
       "      <td>0</td>\n",
       "      <td>app fricken stupid.it froze kindle wont allow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>Please add me!!!!! I need neighbors! Ginger101...</td>\n",
       "      <td>1</td>\n",
       "      <td>please add ! ! ! ! ! need neighbor ! ginger101...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>love it!  this game. is awesome. wish it had m...</td>\n",
       "      <td>1</td>\n",
       "      <td>love ! game . awesome . wish free stuff house ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>I love love love this app on my side of fashio...</td>\n",
       "      <td>1</td>\n",
       "      <td>love love love app side fashion story fight wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>\"This game is a rip off. Here is a list of thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>`` game rip . list thing make better &amp; bull ; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     reviewText,Positive  Positive  \\\n",
       "0      This is a one of the best apps acording to a b...         1   \n",
       "1      This is a pretty good version of the game for ...         1   \n",
       "2      this is a really cool game. there are a bunch ...         1   \n",
       "3      \"This is a silly game and can be frustrating, ...         1   \n",
       "4      This is a terrific game on any pad. Hrs of fun...         1   \n",
       "...                                                  ...       ...   \n",
       "19995  this app is fricken stupid.it froze on the kin...         0   \n",
       "19996  Please add me!!!!! I need neighbors! Ginger101...         1   \n",
       "19997  love it!  this game. is awesome. wish it had m...         1   \n",
       "19998  I love love love this app on my side of fashio...         1   \n",
       "19999  \"This game is a rip off. Here is a list of thi...         0   \n",
       "\n",
       "                                              reviewText  \n",
       "0      one best apps acording bunch people agree bomb...  \n",
       "1      pretty good version game free . lot different ...  \n",
       "2      really cool game . bunch level find golden egg...  \n",
       "3      `` silly game frustrating , lot fun definitely...  \n",
       "4      terrific game pad . hr fun . grandkids love . ...  \n",
       "...                                                  ...  \n",
       "19995  app fricken stupid.it froze kindle wont allow ...  \n",
       "19996  please add ! ! ! ! ! need neighbor ! ginger101...  \n",
       "19997  love ! game . awesome . wish free stuff house ...  \n",
       "19998  love love love app side fashion story fight wo...  \n",
       "19999  `` game rip . list thing make better & bull ; ...  \n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create preprocess_text function\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "# apply the function df\n",
    "df['reviewText'] = df.iloc[:,0].apply(preprocess_text)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fa457c-23e2-43ed-b2d5-484c0f37c8a6",
   "metadata": {},
   "source": [
    "Once our data is normalized, we initialize the sentiment analyzer, and define a function get_sentiment() to call the polarity_scores method on our cleaned column. polarity_scores() gives us the output that lies between [-1,1], where -1 refers to negative sentiment and +1 refers to positive sentiment. We store the predicted values in the Sentiment column in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "039ed5ff-5e92-416f-9c9e-fbe52ad2cfd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText,Positive</th>\n",
       "      <th>Positive</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a one of the best apps acording to a b...</td>\n",
       "      <td>1</td>\n",
       "      <td>one best apps acording bunch people agree bomb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a pretty good version of the game for ...</td>\n",
       "      <td>1</td>\n",
       "      <td>pretty good version game free . lot different ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is a really cool game. there are a bunch ...</td>\n",
       "      <td>1</td>\n",
       "      <td>really cool game . bunch level find golden egg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"This is a silly game and can be frustrating, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>`` silly game frustrating , lot fun definitely...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a terrific game on any pad. Hrs of fun...</td>\n",
       "      <td>1</td>\n",
       "      <td>terrific game pad . hr fun . grandkids love . ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>this app is fricken stupid.it froze on the kin...</td>\n",
       "      <td>0</td>\n",
       "      <td>app fricken stupid.it froze kindle wont allow ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>Please add me!!!!! I need neighbors! Ginger101...</td>\n",
       "      <td>1</td>\n",
       "      <td>please add ! ! ! ! ! need neighbor ! ginger101...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>love it!  this game. is awesome. wish it had m...</td>\n",
       "      <td>1</td>\n",
       "      <td>love ! game . awesome . wish free stuff house ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>I love love love this app on my side of fashio...</td>\n",
       "      <td>1</td>\n",
       "      <td>love love love app side fashion story fight wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>\"This game is a rip off. Here is a list of thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>`` game rip . list thing make better &amp; bull ; ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     reviewText,Positive  Positive  \\\n",
       "0      This is a one of the best apps acording to a b...         1   \n",
       "1      This is a pretty good version of the game for ...         1   \n",
       "2      this is a really cool game. there are a bunch ...         1   \n",
       "3      \"This is a silly game and can be frustrating, ...         1   \n",
       "4      This is a terrific game on any pad. Hrs of fun...         1   \n",
       "...                                                  ...       ...   \n",
       "19995  this app is fricken stupid.it froze on the kin...         0   \n",
       "19996  Please add me!!!!! I need neighbors! Ginger101...         1   \n",
       "19997  love it!  this game. is awesome. wish it had m...         1   \n",
       "19998  I love love love this app on my side of fashio...         1   \n",
       "19999  \"This game is a rip off. Here is a list of thi...         0   \n",
       "\n",
       "                                              reviewText  Sentiment  \n",
       "0      one best apps acording bunch people agree bomb...          1  \n",
       "1      pretty good version game free . lot different ...          1  \n",
       "2      really cool game . bunch level find golden egg...          1  \n",
       "3      `` silly game frustrating , lot fun definitely...          1  \n",
       "4      terrific game pad . hr fun . grandkids love . ...          1  \n",
       "...                                                  ...        ...  \n",
       "19995  app fricken stupid.it froze kindle wont allow ...          0  \n",
       "19996  please add ! ! ! ! ! need neighbor ! ginger101...          1  \n",
       "19997  love ! game . awesome . wish free stuff house ...          1  \n",
       "19998  love love love app side fashion story fight wo...          1  \n",
       "19999  `` game rip . list thing make better & bull ; ...          1  \n",
       "\n",
       "[20000 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize NLTK sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# create get_sentiment function\n",
    "def get_sentiment(text):\n",
    "\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    sentiment = 1 if scores['pos'] > 0 else 0\n",
    "    return sentiment\n",
    "\n",
    "# apply get_sentiment function\n",
    "df['Sentiment'] = df.iloc[:,0].apply(get_sentiment)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cfee2b-4602-46c6-a253-ae48a9d4d868",
   "metadata": {},
   "source": [
    "Now that we have our actual and predicted values, we can use the confusion_matrix function from sklearn library to find our true positives and true negatives and the accuracy_score function from sklearn library to find the accuracy score to see how well SentimentIntensityAnalyzer() predicted the positives of sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a6f0903-6b7a-4cf4-baef-a2627c577cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1456  3311]\n",
      " [  750 14483]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(df.iloc[:,1], df.iloc[:,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840490b8-5b67-41eb-9cba-94323575efe4",
   "metadata": {},
   "source": [
    "From the above output of confusion matrix, our true positives are 14483 and true negatives are 1456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92617a3b-f624-4f12-92cb-f4f055c3b3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79695\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(df.iloc[:,1], df.iloc[:,3])  \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222999fe-e408-44ee-8ebb-d122ae29e6fd",
   "metadata": {},
   "source": [
    "And the accuracy_score shows that the accuracy of SentimentIntensityAnalyzer() if 79.6% which looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ffad90-e971-4001-8533-9e877651012a",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://www.nltk.org/index.html\n",
    "<br>\n",
    "https://medium.com/featurepreneur/simple-sentiment-analysis-with-nlp-vader-400276c7574d\n",
    "<br>\n",
    "https://www.youtube.com/watch?v=FLZvOKSCkxY&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073789c5-5655-4350-b5d2-d2074fcd6bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aap]",
   "language": "python",
   "name": "conda-env-aap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
